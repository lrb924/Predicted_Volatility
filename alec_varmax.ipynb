{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Modules \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX, VARMAXResults\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one of the indices and clean the data\n",
    "def load_index(path, stdev_window=5, mean_window=5):\n",
    "    \n",
    "    df = pd.read_csv(path, parse_dates=True, infer_datetime_format=True, index_col=\"Date\" )\n",
    "\n",
    "    # Compute stdev\n",
    "    df['stdev'] = df['Close'].pct_change().rolling(window=stdev_window).std()\n",
    "    \n",
    "    # Smooth the data\n",
    "    for column in df:    \n",
    "        df[column] = df[column].rolling(window=mean_window).mean()  \n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    # print(df.shape)\n",
    "    print('Data loaded')\n",
    "    # display(df.head(3))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# index_path = './DATA/INDICES/SP_OHLCV.csv'\n",
    "# stdev_window = 25\n",
    "# mean_window = 10\n",
    "# df_SP = load_index(index_path, stdev_window, mean_window)\n",
    "# df_SP.stdev.plot(figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting grids with matplotlib\n",
    "def plot_grid(df, nrows, ncols):\n",
    "    \n",
    "    fix, axes = plt.subplots(nrows=nrows, ncols=ncols, dpi=120, figsize=(10,6))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        data = df[df.columns[i]]\n",
    "        ax.plot(data, linewidth=1)\n",
    "        \n",
    "        ax.set_title(df.columns[i])\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        ax.spines['top'].set_alpha(0)\n",
    "        ax.tick_params(labelsize=6)\n",
    "        \n",
    "    plt.tight_layout();\n",
    "    \n",
    "# Plot dataset\n",
    "# plot_grid(df_SP[['Close', 'stdev', 'Volume']], 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stationarity\n",
    "def check_stationarity(df, n_diffs=0):\n",
    "    \n",
    "    n_diffs = n_diffs\n",
    "    non_stationary = []\n",
    "    \n",
    "    for column in df:\n",
    "        adfuller_res = adfuller(df[column][1:])\n",
    "        adf_stat, p_value = adfuller_res[0], adfuller_res[1]\n",
    "        ci_1, ci_5 = adfuller_res[-2]['1%'], adfuller_res[-2]['5%']\n",
    "        # print(f'ADF Statistic ({column}): {adf_stat}')\n",
    "        # print(f'p-value: {p_value}')\n",
    "        if (adf_stat > ci_1) or (adf_stat > ci_5):\n",
    "            non_stationary.append(column)\n",
    "            # print(f'{column} is non stationary.')\n",
    "        # else:\n",
    "            # print(f'{column} is stationary!')\n",
    "\n",
    "    if len(non_stationary) == 0:\n",
    "        # print(f'All columns are stationary!')\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        df = pd.concat([df[non_stationary].diff(), df.drop(columns=non_stationary)], axis=1).dropna()\n",
    "        n_diffs += 1\n",
    "        \n",
    "        # print(f'Non-stationary columns still exist\\nPerforming .diff(): count {n_diffs}')\n",
    "        return check_stationarity(df, n_diffs), n_diffs\n",
    "    \n",
    "# df_SP, n_diffs  = check_stationarity(df_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data with MaxAbsScaler\n",
    "def scale_data(df, n_diffs, n_train, stdev_window, mean_window):\n",
    "\n",
    "    # Create scaler\n",
    "    scaler = MaxAbsScaler()\n",
    "    scaler.fit(df)\n",
    "    df_scaled = scaler.transform(df)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df.columns).dropna()\n",
    "\n",
    "    # Split data\n",
    "    n_test = len(df) - n_train - n_diffs - stdev_window - mean_window\n",
    "    df_train = df_scaled.iloc[:n_train]\n",
    "    df_test = df_scaled.iloc[n_train:n_test+n_train]\n",
    "\n",
    "    # Create separate test set to preserve index for plotting predictions later\n",
    "    df_pred_index = df.iloc[n_train:n_test+n_train]\n",
    "\n",
    "    # print(f'Number of training days: {n_train}')\n",
    "    print(f'Number of testing days: {n_test}')\n",
    "\n",
    "    # display(df_train.head(3))\n",
    "    # display(df_test.head(3))\n",
    "    \n",
    "    return df_train, df_test, df_pred_index, n_test\n",
    "    \n",
    "# n_train = 2500\n",
    "# df_train, df_test, df_pred_index, n_test = scale_data(df_SP, n_diffs, n_train, stdev_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://goldinlocks.github.io/Multivariate-time-series-models/\n",
    "# \n",
    "# Create VARMAX model & find the best p,q combination\n",
    "# NOTE: WARNING!!!, this might take a LONG time to run!!!\n",
    "\n",
    "def check_p_q(df_train, n_tocheck):\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    # How many values to check for p & q\n",
    "    n_tocheck += 1\n",
    "    \n",
    "    for p in range(n_tocheck):\n",
    "        for q in range(n_tocheck):\n",
    "            if p == 0 and q == 0:\n",
    "                continue\n",
    "                \n",
    "            print(f'Testing order: p = {p}, q = {q}..')\n",
    "            convergence_error, stationarity_error = 0, 0\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                model = VARMAX(\n",
    "                    df_train,\n",
    "                    order = (p,q),\n",
    "                    # trend = 'n',\n",
    "                    filter_concentrated=True\n",
    "                )\n",
    "                model_result = model.fit(maxiter=1000, disp=False)\n",
    "                    \n",
    "            except np.linalg.LinAlgError:\n",
    "                convergence_error += 1\n",
    "                    \n",
    "            except ValueError:\n",
    "                stationarity_error += 1\n",
    "                    \n",
    "            # print('\\nAIC:', model_result.aic)\n",
    "            # print('BIC:', model_result.bic)\n",
    "            # print('HQIC:', model_result.hqic)\n",
    "            # print('------------------------')\n",
    "\n",
    "            test_results[(p, q)] = [model_result.aic,\n",
    "                                    model_result.bic,\n",
    "                                    convergence_error,\n",
    "                                    stationarity_error]\n",
    "    \n",
    "    print('Done testing.')\n",
    "    return test_results\n",
    "\n",
    "# test_results = check_p_q(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://goldinlocks.github.io/Multivariate-time-series-models/\n",
    "\n",
    "def analyze_order(test_results):\n",
    "    \n",
    "    # Create test results dataframe\n",
    "    test_results = pd.DataFrame(test_results).T\n",
    "    test_results.columns = ['AIC', 'BIC', 'convergence', 'stationarity']\n",
    "    test_results.index.names = ['p', 'q']\n",
    "    # test_results.info()\n",
    "    \n",
    "    # We want to minimize BIC\n",
    "    # Visualize the values with a heatmap\n",
    "    sns.heatmap(test_results.BIC.unstack(), fmt='.2f', annot=True, cmap='Blues_r')\n",
    "    b, t = plt.ylim() \n",
    "    b += 0.5 \n",
    "    t -= 0.5 \n",
    "    plt.ylim(b, t) \n",
    "    plt.show()\n",
    "    \n",
    "    # Get the best model\n",
    "    # display(test_results.sort_values('BIC').head(3))\n",
    "    p_best,q_best = test_results.sort_values('BIC').iloc[0].name\n",
    "    print(f'Best p-value: {p_best}')\n",
    "    print(f'Best q-value: {q_best}')\n",
    "    \n",
    "    return p_best, q_best\n",
    "\n",
    "# p_best, q_best = analyze_order(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_varmax(df_train, p_best, q_best):\n",
    "\n",
    "    # Split exogenous and endogenous variables\n",
    "    # endog = df_train.drop(columns=['Volume'])\n",
    "    # exog = df_train['Volume']\n",
    "\n",
    "    # Create VARMAX Model\n",
    "    model = VARMAX(\n",
    "        df_train,\n",
    "        order=(p_best,q_best),\n",
    "        # trend = 'n',\n",
    "        # order=(1,2),\n",
    "        # error_cov_type='diagonal',\n",
    "        filter_concentrated=True\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    print('Fitting model.. Please wait..')\n",
    "    model_fit = model.fit(disp=True)\n",
    "    # model_fit.summary()\n",
    "    \n",
    "    return model_fit\n",
    "\n",
    "# model_fit = create_varmax(df_train, p_best, q_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictions(model_fit, df_test, df_pred_index, n_test):\n",
    "\n",
    "    # Set random seed\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Predicted vs observed volatility\n",
    "    pred = pd.DataFrame(index=df_pred_index.index)\n",
    "    pred['prediction'] = model_fit.simulate(n_test).set_index(pred.index, drop=True).stdev\n",
    "    pred['observed'] = df_test.set_index(pred.index, drop=True).stdev\n",
    "\n",
    "    # Plot error\n",
    "    pred['prediction - observed'] = pred['prediction'] - pred['observed']\n",
    "    pred.plot(figsize=(20,5),title='volatility forecast error',color=['blue','purple','green'],style=['-','-',':'])\n",
    "    plt.legend(loc=('upper left'));\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# pred = analyze_predictions(model_fit, df_pred_index, n_test, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_error(pred):\n",
    "    \n",
    "    error_total = np.round(np.sum(np.abs(pred['prediction - observed'])), 2)\n",
    "    # print(f'Total Error: {error_total}')\n",
    "    \n",
    "    return error_total\n",
    "    \n",
    "# error_total = find_error(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternate version of the function to check for best rolling windows:\n",
    "# ---------------------------------------------\n",
    "# Results of Stdev window test:\n",
    "# 35 - 46.21\n",
    "# 40 - 37.71\n",
    "# 45 - 37.52\n",
    "# 50 - 44.29\n",
    "# 45 gives lowest error\n",
    "# ---------------------------------------------\n",
    "# Results of Mean window test:\n",
    "# 25 - 35.88\n",
    "# 30 - 27.75\n",
    "# 35 - 25.82\n",
    "# 40 - 45.17\n",
    "# 45 - 20.28\n",
    "# 50 - 43.24\n",
    "# 45 gives best error\n",
    "# ---------------------------------------------\n",
    "\n",
    "def run_varmax_predictions_test(stdev_windows, mean_windows):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for stdev_window in stdev_windows:\n",
    "        for mean_window in mean_windows:\n",
    "            \n",
    "            print('-----------------------------')\n",
    "            print(f'Stdev window: {stdev_window}')\n",
    "            print(f'Mean window: {mean_window}')\n",
    "            \n",
    "            path = './DATA/INDICES/SP_OHLCV.csv'\n",
    "\n",
    "            df_SP = load_index(path, stdev_window, mean_window)\n",
    "            # plot_grid(df_SP[['Close', 'stdev', 'Volume']], 3, 1)\n",
    "\n",
    "            df_SP, n_diffs  = check_stationarity(df_SP)\n",
    "\n",
    "            n_train = 2500\n",
    "            df_train, df_test, df_pred_index, n_test = scale_data(df_SP, n_diffs, n_train, stdev_window, mean_window)\n",
    "\n",
    "            # Uncomment these lines below to check for best p & q\n",
    "            # It will take a long time\n",
    "            # --------------------------------------------------\n",
    "            # test_results = check_p_q(df_train, 3)\n",
    "            # p_best, q_best = analyze_order(test_results)\n",
    "\n",
    "            # Setting p, q manually - 1,2 found to give best fit\n",
    "            p_best, q_best = 1, 2\n",
    "            model_fit = create_varmax(df_train, p_best, q_best)\n",
    "\n",
    "            # Plot stdev diagnostics\n",
    "            # model_fit.plot_diagnostics(5,figsize=(20,8));\n",
    "\n",
    "            pred = analyze_predictions(model_fit, df_test, df_pred_index, n_test)\n",
    "            error_total = find_error(pred)\n",
    "            \n",
    "            results[(stdev_window, mean_window)] = error_total\n",
    "            \n",
    "            # print(f'Mean window: {mean_window}')\n",
    "            # print(f'Stdev window: {stdev_window}')\n",
    "            print(f'Total error: {error_total}')\n",
    "            \n",
    "    return results\n",
    "\n",
    "# stdev_windows = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "# mean_windows = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "# # mean_windows, stdev_windows = [5], [5]\n",
    "# results = run_varmax_predictions_test(stdev_windows, mean_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Run Everything\n",
    "# ==================================================\n",
    "\n",
    "def run_varmax_predictions():\n",
    "    \n",
    "    path = './DATA/INDICES/SP_OHLCV.csv'\n",
    "    stdev_window = 45\n",
    "    mean_window = 45\n",
    "\n",
    "    df_SP = load_index(path, stdev_window, mean_window)\n",
    "    plot_grid(df_SP[['Close', 'stdev', 'Volume']], 3, 1)\n",
    "\n",
    "    df_SP, n_diffs  = check_stationarity(df_SP)\n",
    "\n",
    "    n_train = 2500\n",
    "    df_train, df_test, df_pred_index, n_test = scale_data(df_SP, n_diffs, n_train, stdev_window, mean_window)\n",
    "\n",
    "    # Uncomment these lines below to check for best p & q\n",
    "    # It will take a long time\n",
    "    # --------------------------------------------------\n",
    "    # test_results = check_p_q(df_train, 3)\n",
    "    # p_best, q_best = analyze_order(test_results)\n",
    "\n",
    "    # Setting p, q manually - 1,2 found to give best fit\n",
    "    p_best, q_best = 1, 2\n",
    "    model_fit = create_varmax(df_train, p_best, q_best)\n",
    "\n",
    "    # Plot stdev diagnostics\n",
    "    model_fit.plot_diagnostics(5,figsize=(20,8));\n",
    "\n",
    "    pred = analyze_predictions(model_fit, df_test, df_pred_index, n_test)\n",
    "    error_total = find_error(pred)\n",
    "    print(f'Total error: {error_total}')\n",
    "    \n",
    "    # return pred\n",
    "    \n",
    "run_varmax_predictions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc376acbf167264fe84e1585174201a8740edca69fb15bd753e6afd1872b7742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
